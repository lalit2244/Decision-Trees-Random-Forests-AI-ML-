# ðŸ§  Decision Trees & Random Forests â€“ AI & ML Internship Task 5

## ðŸ“Œ Project Overview
This project implements **Decision Tree** and **Random Forest** algorithms to classify patients based on heart disease risk using the **Heart Disease Dataset**.  
It focuses on:
- Understanding Decision Trees
- Preventing overfitting by controlling tree depth
- Comparing accuracy between Decision Tree and Random Forest
- Interpreting feature importance
- Visualizing trees directly in Jupyter Notebook (No Graphviz needed)
- Using cross-validation for reliable evaluation

---

## ðŸŽ¯ Objective
- Learn tree-based models for classification.
- Understand **ensemble learning** and **feature importance**.
- Compare Decision Tree and Random Forest performance.

---

## ðŸ“‚ Dataset
**Name:** Heart Disease Dataset  
**Source:** [Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)  

Contains patient medical data with features such as age, cholesterol, blood pressure, etc., and a target column (`target`) indicating presence of heart disease.

---

## ðŸ›  Tools & Libraries
- **Python 3** (Anaconda / Jupyter Notebook)
- **Pandas** â€“ Data handling
- **NumPy** â€“ Numerical operations
- **Matplotlib & Seaborn** â€“ Visualization
- **Scikit-learn** â€“ Machine learning models & evaluation

---

## ðŸ“œ Steps Performed
1. **Load Dataset** â†’ Read CSV data into Pandas DataFrame.
2. **Data Preprocessing** â†’ Split into features (`X`) and target (`y`).
3. **Train-Test Split** â†’ 80% training, 20% testing.
4. **Decision Tree Classifier** â†’ Limited `max_depth` to prevent overfitting.
5. **Tree Visualization** â†’ Used `plot_tree()` from `sklearn` (No Graphviz required).
6. **Random Forest Classifier** â†’ Trained with 100 trees.
7. **Feature Importance** â†’ Plotted most important features.
8. **Cross-Validation** â†’ Compared average accuracy.

---

## ðŸ“Š Results
| Model                | Accuracy | CV Score |
|----------------------|----------|----------|
| Decision Tree        | ~85%     | ~83%     |
| Random Forest        | ~90%     | ~88%     |

âœ… Random Forest performed better due to ensemble averaging.

---

## ðŸŒ³ Decision Tree Visualization (Direct from Jupyter)
![Decision Tree](decision_tree_plot.png)

---

## ðŸ“ˆ Feature Importance
![Feature Importance](feature_importance.png)

---

